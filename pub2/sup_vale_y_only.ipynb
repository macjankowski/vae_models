{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal = tf.distributions.Normal\n",
    "Bernoulli = tf.distributions.Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Eager API\n",
    "#print(\"Setting Eager mode...\")\n",
    "#tf.enable_eager_execution()\n",
    "#tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "X = np.reshape(X, (X.shape[0], -1))\n",
    "y = np.concatenate((y_train, y_test)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_reshaped = y.reshape(len(y), 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "onehot_encoded = onehot_encoder.fit_transform(label_reshaped)\n",
    "\n",
    "X = (X > 0.5).astype(np.float32)\n",
    "\n",
    "data = np.concatenate((X, onehot_encoded), axis=1)\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=40)\n",
    "train_data=train_data.astype('float')\n",
    "test_data=test_data.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dim=onehot_encoded.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_folder():\n",
    "    now = datetime.datetime.now()\n",
    "    day_of_the_year=now.strftime(\"%d_%m_%Y\")\n",
    "    suff = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "    #print(now.strftime(\"%Y%m%d%H%M%S\"))\n",
    "    log_folder = \"/tmp/class_vae/deep/{}/{}\".format(day_of_the_year,suff)\n",
    "    return log_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, f=tf.nn.relu, name=\"dense\"):\n",
    "        self.name = name\n",
    "        with tf.name_scope(self.name):\n",
    "            self.f = f\n",
    "            self.W = tf.Variable(tf.truncated_normal(shape=(in_dim, out_dim), stddev=0.1), name=\"W\")\n",
    "            self.b = tf.Variable(tf.constant(0.1, shape=[out_dim]), name=\"bias\")\n",
    "            tf.summary.histogram(\"weights\", self.W)\n",
    "            tf.summary.histogram(\"bias\", self.b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        with tf.name_scope(self.name):\n",
    "            act = self.f(tf.matmul(X, self.W) + self.b)\n",
    "            tf.summary.histogram(\"activation\", act)\n",
    "            return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VClassifier:\n",
    "\n",
    "    def encode(self, X, input_dim, hidden_dims):\n",
    "        encoder_layers = []\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dims[:-1]:\n",
    "            h = DenseLayer(in_dim, h_dim)\n",
    "            encoder_layers.append(h)\n",
    "            in_dim = h_dim\n",
    "\n",
    "        middle_layer_dim = hidden_dims[-1]\n",
    "        encoder_layers.append(DenseLayer(in_dim, 2 * middle_layer_dim, f=lambda x: x))\n",
    "\n",
    "        current_value = X\n",
    "        for layer in encoder_layers:\n",
    "            current_value = layer.forward(current_value)\n",
    "\n",
    "        means = current_value[:, :middle_layer_dim]\n",
    "        stdevs = tf.nn.softplus(current_value[:, middle_layer_dim:]) + 1e-6\n",
    "        return means, stdevs\n",
    "\n",
    "    def decode(self, Z, output_dim, hidden_dims):\n",
    "        decoder_layers = []\n",
    "\n",
    "        in_dim = hidden_dims[-1]\n",
    "        for hidden_dim in reversed(hidden_dims[:-1]):\n",
    "            h = DenseLayer(in_dim, hidden_dim)\n",
    "            decoder_layers.append(h)\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        decoder_layers.append(DenseLayer(in_dim, output_dim, f=lambda x: x))\n",
    "\n",
    "        current_value = Z\n",
    "        for decoder_layer in decoder_layers:\n",
    "            current_value = decoder_layer.forward(current_value)\n",
    "\n",
    "        return current_value\n",
    "    \n",
    "    def calculateKL(self, mean, std):\n",
    "        inner = 1 + tf.math.log(std) - mean ** 2 - std ** 2\n",
    "        kls = tf.math.reduce_sum(inner, axis=1)\n",
    "        return tf.math.reduce_mean(kls)\n",
    "    \n",
    "    def create_accuracy_node(self, y_true, y_pred):\n",
    "        correct_mask_node = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "        accuracy_node = tf.reduce_mean(tf.cast(correct_mask_node, tf.float32))\n",
    "        return accuracy_node\n",
    "\n",
    "    def __init__(self, x_dim, y_dim, hidden_dims, log_folder):\n",
    "        self.x_dim = x_dim\n",
    "        self.xy_dim = x_dim + y_dim\n",
    "        tf.reset_default_graph()\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, x_dim), name=\"x\")\n",
    "        self.y = tf.placeholder(tf.float32, shape=(None, y_dim), name=\"xy\")\n",
    "\n",
    "        # with tf.name_scope('input_reshape'):\n",
    "        #    image_shaped_input = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "        #    tf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "        #encoder\n",
    "        means, stdevs = self.encode(self.X, x_dim, hidden_dims)\n",
    "\n",
    "        n = Normal(\n",
    "          loc=means,\n",
    "          scale=stdevs,\n",
    "        )\n",
    "        Z = n.sample()\n",
    "\n",
    "        #decoder\n",
    "        self.logits = self.decode(Z, y_dim, hidden_dims)\n",
    "\n",
    "        self.Y_hat_distribution = Bernoulli(logits=self.logits)\n",
    "        #self.posterior_predictive = self.Y_hat_distribution.sample()\n",
    "\n",
    "        #with tf.name_scope('sample_output_reshaped'):\n",
    "        #    posterior_predictive_reshaped = tf.reshape(self.posterior_predictive, [-1, 28, 28, 1])\n",
    "        #    tf.summary.image('sample_output', tf.cast(posterior_predictive_reshaped, tf.float32), 10)\n",
    "\n",
    "        self.posterior_predictive_probs = tf.nn.sigmoid(self.logits)\n",
    "\n",
    "        # with tf.name_scope('probs_output_reshaped'):\n",
    "        #     posterior_predictive_probs_reshaped = tf.reshape(self.posterior_predictive_probs[:,0:x_dim], [-1, 28, 28, 1])\n",
    "        #     tf.summary.image('probs_output', posterior_predictive_probs_reshaped, 10)\n",
    "\n",
    "        with tf.name_scope('COST'):\n",
    "            expected_log_likelihood = tf.reduce_sum(\n",
    "                self.Y_hat_distribution.log_prob(self.y),\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            #tf.summary.scalar(\"Expected log-likelihood\", tf.reduce_sum(expected_log_likelihood))\n",
    "\n",
    "            kl = self.calculateKL(means, stdevs)\n",
    "            tf.summary.scalar(\"KL\", kl)\n",
    "\n",
    "            exp_loglik = tf.reduce_mean(expected_log_likelihood)\n",
    "            tf.summary.scalar(\"loglik\", exp_loglik)\n",
    "\n",
    "            elbo = exp_loglik #+ kl\n",
    "            tf.summary.scalar(\"ELBO\", elbo)\n",
    "\n",
    "        self.accuracy_node = self.create_accuracy_node(self.y, self.posterior_predictive_probs)\n",
    "        tf.summary.scalar(\"accuracy\", self.accuracy_node)\n",
    "            \n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(-elbo)\n",
    "        #self.train_op = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(-elbo)\n",
    "\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(self.init_op)\n",
    "\n",
    "        self.merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        self.writer_train = tf.summary.FileWriter(log_folder)\n",
    "        self.writer_train.add_graph(self.sess.graph)\n",
    "\n",
    "    def fit(self, X_train, X_test, epochs=30, batch_sz=64):\n",
    "        #costs=[]\n",
    "        n_batches = len(X_train) // batch_sz\n",
    "        print(\"n_batches:\", n_batches)\n",
    "\n",
    "        iter = 1\n",
    "        for i in range(epochs):\n",
    "            print(\"epoch: %d\" % i)\n",
    "            np.random.shuffle(X_train)\n",
    "            for j in range(n_batches):\n",
    "                batch = X_train[j * batch_sz:(j + 1) * batch_sz]\n",
    "                #print(batch.shape)\n",
    "                #print(\"x.shape={}\".format(batch[:,0:784].shape))\n",
    "                #print(\"y.shape={}\".format(batch[:,784:794].shape))\n",
    "                self.sess.run(self.train_op, feed_dict={self.X: batch[:,0:self.x_dim], self.y: batch[:,self.x_dim:self.xy_dim]})\n",
    "                #c /= batch_sz\n",
    "                #costs.append(-c)\n",
    "                if j % 100 == 0:\n",
    "                    s = self.sess.run(self.merged_summary, feed_dict={self.X: batch[:,0:self.x_dim], self.y: batch[:,self.x_dim:self.xy_dim]})\n",
    "                    self.writer_train.add_summary(s, iter)\n",
    "                    #print(\"iter: %d, cost: %.3f\" % (j, c))\n",
    "                iter += 1\n",
    "            train_accuracy = self.calculate_accuracy(X_train, self.x_dim)\n",
    "            print(\"Train accuracy {}\".format(train_accuracy))\n",
    "            test_accuracy = self.calculate_accuracy(X_test, self.x_dim)\n",
    "            print(\"Test accuracy {}\".format(test_accuracy))\n",
    "\n",
    "        # plt.plot(costs)\n",
    "        # plt.show()\n",
    "        \n",
    "    def calculate_accuracy(self, Xy, x_dim):\n",
    "        accuracy = self.sess.run(self.accuracy_node, feed_dict={self.X: Xy[:,0:x_dim], self.y: Xy[:,self.x_dim:self.xy_dim]})\n",
    "        return accuracy\n",
    "        \n",
    "    def predict(self, Xy, x_dim, xy_dim):\n",
    "        y_pred = self.sess.run(self.posterior_predictive_probs_y, feed_dict={self.X: Xy[:,0:x_dim], self.y: Xy[:,self.x_dim:self.xy_dim]})\n",
    "        return y_pred\n",
    "\n",
    "    def predict_probs(self, X):\n",
    "        return self.sess.run(self.posterior_predictive_probs, feed_dict={self.X: X})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting autoencoder. Log folder=/tmp/class_vae/deep/10_02_2019/20190210182124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/cost-prediction-env/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_batches: 984\n",
      "epoch: 0\n",
      "Train accuracy 0.9585396647453308\n",
      "Test accuracy 0.9438571333885193\n",
      "epoch: 1\n",
      "Train accuracy 0.9715079069137573\n",
      "Test accuracy 0.959857165813446\n",
      "epoch: 2\n",
      "Train accuracy 0.9731746315956116\n",
      "Test accuracy 0.9584285616874695\n",
      "epoch: 3\n",
      "Train accuracy 0.9770317673683167\n",
      "Test accuracy 0.9582856893539429\n",
      "epoch: 4\n",
      "Train accuracy 0.9847142696380615\n",
      "Test accuracy 0.9677143096923828\n",
      "epoch: 5\n",
      "Train accuracy 0.9828730225563049\n",
      "Test accuracy 0.9645714163780212\n",
      "epoch: 6\n",
      "Train accuracy 0.9886031746864319\n",
      "Test accuracy 0.9712857007980347\n",
      "epoch: 7\n",
      "Train accuracy 0.9893015623092651\n",
      "Test accuracy 0.9729999899864197\n",
      "epoch: 8\n",
      "Train accuracy 0.9895238280296326\n",
      "Test accuracy 0.9728571176528931\n",
      "epoch: 9\n",
      "Train accuracy 0.990746021270752\n",
      "Test accuracy 0.972000002861023\n"
     ]
    }
   ],
   "source": [
    "log_folder = get_log_folder()\n",
    "print('Starting autoencoder. Log folder={}'.format(log_folder))\n",
    "model = VClassifier(x_dim=X.shape[1], y_dim=y_dim, hidden_dims=[1024, 512, 256, 128, 64], log_folder=log_folder)\n",
    "model.fit(train_data, test_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99073017"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.calculate_accuracy(train_data, x_dim=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97185713"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.calculate_accuracy(test_data, x_dim=X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
